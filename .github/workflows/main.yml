name: Deploy to EKS with Karpenter

on:
  push:
    branches:
      - main

jobs:
  deploy:
    runs-on: ubuntu-latest
    env:
      AWS_REGION: ${{ secrets.AWS_REGION }}
      ECR_REGISTRY: ${{ secrets.ECR_REGISTRY }}
      ECR_REPOSITORY: ${{ secrets.ECR_REPOSITORY }}
      EKS_CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
      EKS_ROLE_ARN: ${{ secrets.EKS_ROLE_ARN }}
      KARPENTER_VERSION: "1.6.0"

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          role-to-assume: ${{ env.EKS_ROLE_ARN }}
          role-skip-session-tagging: true

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build, tag, and push Docker image
        run: |
          docker build -t project/flask:latest .
          docker tag project/flask:latest 574205320701.dkr.ecr.ap-northeast-2.amazonaws.com/project/flask:latest
          docker push 574205320701.dkr.ecr.ap-northeast-2.amazonaws.com/project/flask:latest

      - name: Install aws-iam-authenticator
        run: |
          curl -Lo aws-iam-authenticator https://github.com/kubernetes-sigs/aws-iam-authenticator/releases/download/v0.6.14/aws-iam-authenticator_0.6.14_linux_amd64
          chmod +x ./aws-iam-authenticator
          sudo mv ./aws-iam-authenticator /usr/local/bin

      - name: Install Helm
        uses: azure/setup-helm@v4
        with:
            version: 'latest'

      # kubeconfig 완전 재설정
      - name: Update Kubeconfig
        run: |
          echo "=== 클러스터 정보 확인 ==="
          aws eks describe-cluster --region $AWS_REGION --name $EKS_CLUSTER_NAME --query 'cluster.{Name:name,Status:status,Endpoint:endpoint}'
          
          echo "=== 기존 kubeconfig 정리 ==="
          rm -f ~/.kube/config
          mkdir -p ~/.kube
          
          echo "=== 새로운 kubeconfig 생성 ==="
          aws eks update-kubeconfig --region $AWS_REGION --name $EKS_CLUSTER_NAME --verbose
          
          echo "=== kubeconfig 파일 권한 설정 ==="
          chmod 600 ~/.kube/config
          
          echo "=== kubeconfig 내용 확인 ==="
          kubectl config view --minify
          
          echo "=== 현재 컨텍스트 확인 ==="
          kubectl config current-context
          
          echo "=== 엔드포인트 확인 ==="
          kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}'
          echo

      # 강화된 kubectl 접근 테스트
      - name: Verify kubectl access
        run: |
          echo "=== Current AWS Identity ==="
          aws sts get-caller-identity
          
          echo "=== kubectl 버전 확인 ==="
          kubectl version --client
          
          echo "=== 클러스터 연결 테스트 ==="
          kubectl cluster-info
          
          echo "=== 권한 테스트 ==="
          kubectl auth can-i get pods --all-namespaces || echo "Pod access check failed"
          kubectl auth can-i get namespaces || echo "Namespace access check failed"
          kubectl auth can-i get nodes || echo "Node access check failed"
          
          echo "=== 네임스페이스 목록 ==="
          kubectl get namespaces || echo "Failed to get namespaces"
          
          echo "=== 노드 목록 ==="
          kubectl get nodes || echo "Failed to get nodes"

# 1단계: 기존 역할 활용한 간단한 IAM 설정
      - name: Setup Karpenter with existing IAM role
        run: |
          echo "=== 기존 IAM 역할로 Karpenter 설정 ==="
          
          # 인스턴스 프로파일만 생성 (권한 문제 없음)
          aws iam create-instance-profile --instance-profile-name KarpenterNodeInstanceProfile 2>/dev/null || echo "Instance profile already exists"
          aws iam add-role-to-instance-profile --instance-profile-name KarpenterNodeInstanceProfile --role-name project-node-group 2>/dev/null || echo "Role already added"
          
          echo "=== IAM 설정 완료 ==="

      # 2단계: Karpenter 네임스페이스 및 ServiceAccount 생성 (기존 역할 사용)
      - name: Create Karpenter namespace and ServiceAccount
        run: |
          echo "=== Karpenter 네임스페이스 및 ServiceAccount 생성 ==="
          
          # 기존 정리
          helm uninstall karpenter -n karpenter 2>/dev/null || echo "No existing release"
          kubectl delete namespace karpenter --ignore-not-found=true
          sleep 30
          
          # 네임스페이스 생성
          kubectl create namespace karpenter
          
          # ServiceAccount 생성 (기존 project-node-group 역할 사용)
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: karpenter
            namespace: karpenter
            annotations:
              eks.amazonaws.com/role-arn: arn:aws:iam::574205320701:role/project-node-group
          EOF

      # 3단계: Karpenter 설치 (수정된 버전)
      - name: Install Karpenter step by step
        run: |
          echo "=== Karpenter 단계별 설치 ==="
          
          # Helm 차트 설치 (타임아웃 줄이고 안정적으로)
          echo "Helm 차트 설치 중..."
          helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter \
            --version $KARPENTER_VERSION \
            --namespace karpenter \
            --set "settings.clusterName=$EKS_CLUSTER_NAME" \
            --set "settings.clusterEndpoint=$(aws eks describe-cluster --region $AWS_REGION --name $EKS_CLUSTER_NAME --query 'cluster.endpoint' --output text)" \
            --set "serviceAccount.create=false" \
            --set "serviceAccount.name=karpenter" \
            --set controller.resources.requests.cpu=200m \
            --set controller.resources.requests.memory=200Mi \
            --set controller.resources.limits.cpu=400m \
            --set controller.resources.limits.memory=400Mi \
            --set controller.replicas=1 \
            --timeout 8m0s
          
          echo "설치 완료, 상태 확인 중..."
          sleep 60
          kubectl get pods -n karpenter
          kubectl logs -n karpenter deployment/karpenter --tail=20 || echo "로그 확인 실패"

      # 4단계: Karpenter 상태 확인 및 CRD 대기
      - name: Wait for Karpenter to be ready
        run: |
          echo "=== Karpenter 준비 상태 대기 ==="
          
          # Pod 상태 확인 (최대 10분 대기)
          for i in {1..20}; do
            echo "상태 확인 $i/20..."
            
            # Pod 상태 출력
            kubectl get pods -n karpenter -o wide
            
            # Pod이 Running 상태인지 확인
            if kubectl get pods -n karpenter --field-selector=status.phase=Running 2>/dev/null | grep -q "Running"; then
              echo "✅ Karpenter Pod이 Running 상태입니다!"
              break
            fi
            
            # 문제가 있으면 이벤트 확인
            if [[ $i -eq 10 ]]; then
              echo "=== 중간 진단 ==="
              kubectl describe pods -n karpenter | tail -20
            fi
            
            sleep 30
          done
          
          # CRD 확인
          echo "=== CRD 확인 ==="
          for i in {1..10}; do
            if kubectl get crd nodepools.karpenter.sh 2>/dev/null; then
              echo "✅ NodePool CRD 준비 완료!"
              break
            fi
            echo "CRD 대기 중... ($i/10)"
            sleep 20
          done
          
          # 최종 상태 설정
          if kubectl get pods -n karpenter --field-selector=status.phase=Running 2>/dev/null | grep -q "Running" && kubectl get crd nodepools.karpenter.sh 2>/dev/null; then
            echo "KARPENTER_READY=true" >> $GITHUB_ENV
            echo "✅ Karpenter 설치 및 준비 완료!"
          else
            echo "KARPENTER_READY=false" >> $GITHUB_ENV
            echo "❌ Karpenter 설치 실패 - 노드 그룹 계속 사용"
          fi
          
          echo "=== 최종 상태 요약 ==="
          kubectl get pods -n karpenter || echo "Karpenter Pod 없음"
          kubectl get crd | grep karpenter || echo "Karpenter CRD 없음"
          kubectl get nodes -o wide

      # 4단계: Karpenter 상태 진단 및 대기
      - name: Diagnose and wait for Karpenter
        run: |
          echo "=== Karpenter 상태 진단 ==="
          
          # Pod 상태 확인
          for i in {1..10}; do
            echo "시도 $i/10..."
            kubectl get pods -n karpenter -o wide
            
            # Pod이 Running 상태인지 확인
            if kubectl get pods -n karpenter --field-selector=status.phase=Running | grep -q "Running"; then
              echo "✅ Karpenter Pod이 Running 상태입니다!"
              break
            fi
            
            # Pod 상태가 이상하면 로그 확인
            echo "Pod 상태 확인 중..."
            kubectl describe pods -n karpenter | grep -A 10 "Events:" || echo "이벤트 없음"
            
            sleep 30
          done
          
          # 최종 상태 확인
          echo "=== 최종 상태 ==="
          kubectl get pods -n karpenter
          kubectl get serviceaccounts -n karpenter
          kubectl logs -n karpenter deployment/karpenter --tail=50 || echo "로그 없음"

      # 5단계: CRD 확인 및 생성
      - name: Verify and create Karpenter CRDs
        run: |
          echo "=== Karpenter CRD 확인 ==="
          
          # CRD 존재 확인
          for i in {1..15}; do
            echo "CRD 확인 시도 $i/15..."
            
            if kubectl get crd nodepools.karpenter.sh 2>/dev/null && kubectl get crd ec2nodeclasses.karpenter.k8s.aws 2>/dev/null; then
              echo "✅ 모든 Karpenter CRD가 준비되었습니다!"
              kubectl get crd | grep karpenter
              break
            fi
            
            echo "CRD 대기 중..."
            sleep 20
          done
          
          # CRD가 없으면 수동 생성 시도
          if ! kubectl get crd nodepools.karpenter.sh 2>/dev/null; then
            echo "❌ CRD가 생성되지 않았습니다. Karpenter Pod 재시작..."
            kubectl rollout restart deployment/karpenter -n karpenter
            sleep 60
            kubectl get crd | grep karpenter || echo "CRD 여전히 없음"
          fi

      # 6단계: 강제로 Karpenter 활성화
      - name: Force Karpenter activation
        run: |
          echo "=== Karpenter 강제 활성화 ==="
          
          # Karpenter Pod가 정상인지 최종 확인
          POD_STATUS=$(kubectl get pods -n karpenter --no-headers | awk '{print $3}')
          echo "현재 Pod 상태: $POD_STATUS"
          
          if [[ "$POD_STATUS" != "Running" ]]; then
            echo "Pod이 Running 상태가 아닙니다. 문제 해결 시도..."
            
            # ServiceAccount 재생성
            kubectl delete serviceaccount karpenter -n karpenter --ignore-not-found=true
            cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: karpenter
            namespace: karpenter
            annotations:
              eks.amazonaws.com/role-arn: arn:aws:iam::574205320701:role/KarpenterControllerRole
          EOF
            
            # Deployment 재시작
            kubectl rollout restart deployment/karpenter -n karpenter
            sleep 60
          fi
          
          # 최종 상태
          echo "=== 최종 Karpenter 상태 ==="
          kubectl get pods -n karpenter
          kubectl get crd | grep karpenter
          
          # 환경 변수 설정
          if kubectl get pods -n karpenter --field-selector=status.phase=Running | grep -q "Running" && kubectl get crd nodepools.karpenter.sh 2>/dev/null; then
            echo "KARPENTER_READY=true" >> $GITHUB_ENV
            echo "✅ Karpenter 설치 성공!"
          else
            echo "KARPENTER_READY=false" >> $GITHUB_ENV
            echo "❌ Karpenter 설치 실패"
          fi
  
      - name: Apply Karpenter NodePool configuration
        run: |
            echo "=== NodePool 적용 ==="
            cat <<EOF | kubectl apply -f -
            apiVersion: karpenter.sh/v1beta1
            kind: NodePool
            metadata:
              name: default
            spec:
              template:
                metadata:
                  labels:
                    karpenter.sh/nodepool: default
                spec:
                  requirements:
                    - key: kubernetes.io/arch
                      operator: In
                      values: ["amd64"]
                    - key: kubernetes.io/os
                      operator: In
                      values: ["linux"]
                    - key: karpenter.sh/capacity-type
                      operator: In
                      values: ["spot", "on-demand"]
                    - key: node.kubernetes.io/instance-type
                      operator: In
                      values: ["t3.medium", "t3.large", "t3.xlarge", "m5.large", "m5.xlarge"]
                  
                  nodeClassRef:
                    apiVersion: karpenter.k8s.aws/v1beta1
                    kind: EC2NodeClass
                    name: default
              
              limits:
                cpu: 1000
                memory: 1000Gi
              
              disruption:
                consolidationPolicy: WhenUnderutilized
                consolidateAfter: 30s
                expireAfter: 30m
            EOF
  
      - name: Apply Karpenter EC2NodeClass configuration
        run: |
            echo "=== EC2NodeClass 적용 ==="
            cat <<EOF | kubectl apply -f -
            apiVersion: karpenter.k8s.aws/v1beta1
            kind: EC2NodeClass
            metadata:
              name: default
            spec:
              amiFamily: AL2023
              
              # 실제 서브넷 ID 직접 지정
              subnetSelectorTerms:
                - id: subnet-05debc08a249d8c0d
                - id: subnet-0005ba7fed0b87805
              
              # 실제 보안 그룹 ID 직접 지정
              securityGroupSelectorTerms:
                - id: sg-0f6bacca39650f9e6
              
              role: "KarpenterNodeInstanceProfile"
              
              userData: |
                #!/bin/bash
                /etc/eks/bootstrap.sh $EKS_CLUSTER_NAME
                echo "Node bootstrapped successfully"
              
              blockDeviceMappings:
                - deviceName: /dev/xvda
                  ebs:
                    volumeSize: 20Gi
                    volumeType: gp3
                    encrypted: true
                    deleteOnTermination: true
              
              tags:
                Environment: production
                Team: platform
                Project: flask-app
                karpenter.sh/discovery: $EKS_CLUSTER_NAME
            EOF

      - name: Set secrets for Kubernetes
        run: |
          kubectl delete secret flask-secrets --ignore-not-found
          kubectl create secret generic flask-secrets \
            --from-literal=GEMINI_API_KEY="${{ secrets.GEMINI_API_KEY }}" \
            --from-literal=TAGO_API_KEY="${{ secrets.TAGO_API_KEY }}" \
            --from-literal=API_KEY="${{ secrets.API_KEY }}" \
            --from-literal=FLASK_SECRET_KEY="${{ secrets.FLASK_SECRET_KEY }}" \
            --from-literal=MYSQL_URI="${{ secrets.MYSQL_URI }}" \
            --from-literal=AWS_ACCESS_KEY_ID="${{ secrets.AWS_ACCESS_KEY_ID }}" \
            --from-literal=AWS_SECRET_ACCESS_KEY="${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
            --from-literal=AWS_REGION="${{ secrets.AWS_REGION }}" \
            --from-literal=S3_BUCKET="${{ secrets.S3_BUCKET }}" \
            --from-literal=DYNAMODB_TABLE="${{ secrets.DYNAMODB_TABLE }}"

      - name: Deploy to EKS
        run: |
          kubectl apply -f deployment.yaml
          kubectl apply -f service.yaml

      - name: Wait for deployment to be ready
        run: |
          echo "Waiting for deployment to be ready..."
          kubectl rollout status deployment/flask-app --timeout=300s

      - name: Check Karpenter node provisioning
        run: |
          echo "=== Karpenter Status ==="
          kubectl get nodepool
          kubectl get ec2nodeclass
          kubectl get pods -n karpenter
          echo "=== Node Status ==="
          kubectl get nodes -L karpenter.sh/nodepool
          echo "=== Pending Pods ==="
          kubectl get pods --field-selector=status.phase=Pending

      - name: Wait for LoadBalancer DNS
        id: wait-lb
        run: |
          for i in {1..30}; do
            LB_DNS=$(kubectl get svc flask-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
            if [ -n "$LB_DNS" ]; then
              echo "LB_DNS=$LB_DNS" >> $GITHUB_ENV
              echo "LoadBalancer DNS: $LB_DNS"
              break
            fi
            echo "Waiting for LoadBalancer DNS... ($i/30)"
            sleep 10
          done
          if [ -z "$LB_DNS" ]; then
            echo "LoadBalancer DNS not assigned after 5 minutes!"
            kubectl get svc flask-service -o yaml
            exit 1
          fi

      - name: Health check
        run: |
          echo "Waiting for service to be fully ready..."
          sleep 60
          echo "Running health check on http://$LB_DNS"
          python healthcheck.py http://$LB_DNS

      - name: Show deployment status
        run: |
          echo "=== Deployment Summary ==="
          echo "Application URL: http://$LB_DNS"
          echo "=== Pods Status ==="
          kubectl get pods -l app=flask-app
          echo "=== Service Status ==="
          kubectl get svc flask-service
          echo "=== Deployment Status ==="
          kubectl get deployment flask-app
          echo "=== Karpenter Nodes ==="
          kubectl get nodes -L karpenter.sh/nodepool,node.kubernetes.io/instance-type