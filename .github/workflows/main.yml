name: Deploy to EKS with Karpenter

on:
  push:
    branches:
      - main

jobs:
  deploy:
    runs-on: ubuntu-latest
    env:
      AWS_REGION: ${{ secrets.AWS_REGION }}
      ECR_REGISTRY: ${{ secrets.ECR_REGISTRY }}
      ECR_REPOSITORY: ${{ secrets.ECR_REPOSITORY }}
      EKS_CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}
      EKS_ROLE_ARN: ${{ secrets.EKS_ROLE_ARN }}
      KARPENTER_VERSION: "1.6.0"

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          role-to-assume: ${{ env.EKS_ROLE_ARN }}
          role-skip-session-tagging: true

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build, tag, and push Docker image
        run: |
          docker build -t project/flask:latest .
          docker tag project/flask:latest 574205320701.dkr.ecr.ap-northeast-2.amazonaws.com/project/flask:latest
          docker push 574205320701.dkr.ecr.ap-northeast-2.amazonaws.com/project/flask:latest

      - name: Install aws-iam-authenticator
        run: |
          curl -Lo aws-iam-authenticator https://github.com/kubernetes-sigs/aws-iam-authenticator/releases/download/v0.6.14/aws-iam-authenticator_0.6.14_linux_amd64
          chmod +x ./aws-iam-authenticator
          sudo mv ./aws-iam-authenticator /usr/local/bin

      - name: Install Helm
        uses: azure/setup-helm@v4
        with:
            version: 'latest'

      # kubeconfig ÏôÑÏ†Ñ Ïû¨ÏÑ§Ï†ï
      - name: Update Kubeconfig
        run: |
          echo "==== ÌÅ¥Îü¨Ïä§ÌÑ∞ Ï†ïÎ≥¥ ÌôïÏù∏ ===="
          aws eks describe-cluster --region $AWS_REGION --name $EKS_CLUSTER_NAME --query 'cluster.{Name:name,Status:status,Endpoint:endpoint}'
          
          echo "=== Í∏∞Ï°¥ kubeconfig Ï†ïÎ¶¨ ==="
          rm -f ~/.kube/config
          mkdir -p ~/.kube
          
          echo "=== ÏÉàÎ°úÏö¥ kubeconfig ÏÉùÏÑ± ==="
          aws eks update-kubeconfig --region $AWS_REGION --name $EKS_CLUSTER_NAME --verbose
          
          echo "=== kubeconfig ÌååÏùº Í∂åÌïú ÏÑ§Ï†ï ==="
          chmod 600 ~/.kube/config
          
          echo "=== kubeconfig ÎÇ¥Ïö© ÌôïÏù∏ ==="
          kubectl config view --minify
          
          echo "=== ÌòÑÏû¨ Ïª®ÌÖçÏä§Ìä∏ ÌôïÏù∏ ==="
          kubectl config current-context
          
          echo "=== ÏóîÎìúÌè¨Ïù∏Ìä∏ ÌôïÏù∏ ==="
          kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}'
          echo

      # Í∞ïÌôîÎêú kubectl Ï†ëÍ∑º ÌÖåÏä§Ìä∏
      - name: Verify kubectl access
        run: |
          echo "=== Current AWS Identity ==="
          aws sts get-caller-identity
          
          echo "=== kubectl Î≤ÑÏ†Ñ ÌôïÏù∏ ==="
          kubectl version --client
          
          echo "=== ÌÅ¥Îü¨Ïä§ÌÑ∞ Ïó∞Í≤∞ ÌÖåÏä§Ìä∏ ==="
          kubectl cluster-info
          
          echo "=== Í∂åÌïú ÌÖåÏä§Ìä∏ ==="
          kubectl auth can-i get pods --all-namespaces || echo "Pod access check failed"
          kubectl auth can-i get namespaces || echo "Namespace access check failed"
          kubectl auth can-i get nodes || echo "Node access check failed"
          
          echo "=== ÎÑ§ÏûÑÏä§ÌéòÏù¥Ïä§ Î™©Î°ù ==="
          kubectl get namespaces || echo "Failed to get namespaces"
          
          echo "=== ÎÖ∏Îìú Î™©Î°ù ==="
          kubectl get nodes || echo "Failed to get nodes"

# Í∏∞Ï°¥ ECRÏóê Karpenter Ïù¥ÎØ∏ÏßÄ Ï∂îÍ∞Ä
      - name: Add Karpenter image to existing ECR
        run: |
          echo "=== Í∏∞Ï°¥ project/flask ECRÏóê Karpenter Ïù¥ÎØ∏ÏßÄ Ï∂îÍ∞Ä ==="
          
          # GitHub Actions Îü¨ÎÑàÏóêÏÑú ECR Public Ïù¥ÎØ∏ÏßÄ Îã§Ïö¥Î°úÎìú (ÌçºÎ∏îÎ¶≠ ÎÑ§Ìä∏ÏõåÌÅ¨ ÏÇ¨Ïö©)
          echo "Karpenter Ïù¥ÎØ∏ÏßÄ Îã§Ïö¥Î°úÎìú Ï§ë..."
          docker pull public.ecr.aws/karpenter/controller:v0.34.0
          
          # Í∏∞Ï°¥ Flask ECRÏóê karpenter ÌÉúÍ∑∏Î°ú Ï†ÄÏû•
          echo "Í∏∞Ï°¥ ECRÏóê ÌÉúÍ∑∏ Î∞è Ìë∏Ïãú..."
          docker tag public.ecr.aws/karpenter/controller:v0.34.0 574205320701.dkr.ecr.ap-northeast-2.amazonaws.com/project/flask:karpenter-0.34.0
          docker push 574205320701.dkr.ecr.ap-northeast-2.amazonaws.com/project/flask:karpenter-0.34.0
          
          echo "‚úÖ Karpenter Ïù¥ÎØ∏ÏßÄÍ∞Ä Í∏∞Ï°¥ ECRÏóê Ï∂îÍ∞ÄÎêòÏóàÏäµÎãàÎã§!"
          
          # Ï∂îÍ∞ÄÎêú Ïù¥ÎØ∏ÏßÄ ÌôïÏù∏
          aws ecr describe-images --repository-name project/flask --region ap-northeast-2 --query 'imageDetails[*].imageTags'

      # Karpenter ÏÑ§Ïπò (Í∏∞Ï°¥ ECR Ïù¥ÎØ∏ÏßÄ ÏÇ¨Ïö©)
      - name: Install Karpenter with existing ECR
        run: |
          echo "=== Í∏∞Ï°¥ ECR Ïù¥ÎØ∏ÏßÄÎ°ú Karpenter ÏÑ§Ïπò ==="
          
          # Í∏∞Ï°¥ Ï†ïÎ¶¨
          helm uninstall karpenter -n karpenter 2>/dev/null || echo "No existing release"
          kubectl delete namespace karpenter --ignore-not-found=true
          sleep 30
          
          # Karpenter ÏÑ§Ï†ï
          kubectl create namespace karpenter
          
          # ServiceAccount ÏÉùÏÑ± (Í∏∞Ï°¥ project-node-group Ïó≠Ìï† ÏÇ¨Ïö©)
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: karpenter
            namespace: karpenter
            annotations:
              eks.amazonaws.com/role-arn: arn:aws:iam::574205320701:role/project-node-group
          EOF
          
          # Ïù∏Ïä§ÌÑ¥Ïä§ ÌîÑÎ°úÌååÏùº ÏÑ§Ï†ï
          aws iam create-instance-profile --instance-profile-name KarpenterNodeInstanceProfile 2>/dev/null || echo "Instance profile already exists"
          aws iam add-role-to-instance-profile --instance-profile-name KarpenterNodeInstanceProfile --role-name project-node-group 2>/dev/null || echo "Role already added"
          
          # Í∏∞Ï°¥ ECR Ïù¥ÎØ∏ÏßÄÎ°ú Karpenter ÏÑ§Ïπò
          echo "HelmÏúºÎ°ú Karpenter ÏÑ§Ïπò Ï§ë..."
          helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter \
            --version $KARPENTER_VERSION \
            --namespace karpenter \
            --set "settings.clusterName=$EKS_CLUSTER_NAME" \
            --set "settings.clusterEndpoint=$(aws eks describe-cluster --region $AWS_REGION --name $EKS_CLUSTER_NAME --query 'cluster.endpoint' --output text)" \
            --set "serviceAccount.create=false" \
            --set "serviceAccount.name=karpenter" \
            --set controller.image.repository=574205320701.dkr.ecr.ap-northeast-2.amazonaws.com/project/flask \
            --set controller.image.tag=karpenter-0.34.0 \
            --set controller.image.digest=sha256:4263bb55b891c19896e1f6d70a993d242a465be01bebaa743199c26c3803d41b \
            --set controller.resources.requests.cpu=200m \
            --set controller.resources.requests.memory=200Mi \
            --set controller.resources.limits.cpu=400m \
            --set controller.resources.limits.memory=400Mi \
            --set controller.replicas=1 \
            --timeout 8m0s
          
          echo "=== ÏÑ§Ïπò ÏôÑÎ£å! ==="

      # Karpenter ÏÉÅÌÉú ÌôïÏù∏ Î∞è Ï§ÄÎπÑ ÎåÄÍ∏∞
      - name: Verify Karpenter installation
        run: |
          echo "=== Karpenter ÏÑ§Ïπò ÌôïÏù∏ ==="
          
          # ÏÑ§Ïπò ÌõÑ Ïû†Ïãú ÎåÄÍ∏∞
          sleep 60
          
          # Pod ÏÉÅÌÉú ÌôïÏù∏
          echo "Pod ÏÉÅÌÉú:"
          kubectl get pods -n karpenter -o wide
          
          # PodÏù¥ Running ÏÉÅÌÉúÍ∞Ä Îê† ÎïåÍπåÏßÄ ÎåÄÍ∏∞ (ÏµúÎåÄ 10Î∂Ñ)
          echo "=== Karpenter Pod Ï§ÄÎπÑ ÎåÄÍ∏∞ ==="
          for i in {1..20}; do
            echo "ÏÉÅÌÉú ÌôïÏù∏ $i/20..."
            
            if kubectl get pods -n karpenter --field-selector=status.phase=Running 2>/dev/null | grep -q "Running"; then
              echo "‚úÖ Karpenter PodÏù¥ Running ÏÉÅÌÉúÏûÖÎãàÎã§!"
              break
            fi
            
            # Ï§ëÍ∞Ñ ÏßÑÎã®
            if [[ $i -eq 10 ]]; then
              echo "=== Ï§ëÍ∞Ñ ÏÉÅÌÉú ÏßÑÎã® ==="
              kubectl describe pods -n karpenter | tail -30
            fi
            
            sleep 30
          done
          
          # CRD ÌôïÏù∏
          echo "=== CRD ÎåÄÍ∏∞ Î∞è ÌôïÏù∏ ==="
          for i in {1..10}; do
            if kubectl get crd nodepools.karpenter.sh 2>/dev/null; then
              echo "‚úÖ NodePool CRD Ï§ÄÎπÑ ÏôÑÎ£å!"
              break
            fi
            echo "CRD ÎåÄÍ∏∞ Ï§ë... ($i/10)"
            sleep 20
          done
          
          # ÏµúÏ¢Ö ÏÉÅÌÉú ÏÑ§Ï†ï
          echo "=== ÏµúÏ¢Ö Í≤∞Í≥º ==="
          if kubectl get pods -n karpenter --field-selector=status.phase=Running 2>/dev/null | grep -q "Running" && kubectl get crd nodepools.karpenter.sh 2>/dev/null; then
            echo "KARPENTER_READY=true" >> $GITHUB_ENV
            echo "üéâ Karpenter ÏÑ§Ïπò Î∞è Ï§ÄÎπÑ ÏôÑÎ£å!"
            echo "ÏÇ¨Ïö©Îêú Ïù¥ÎØ∏ÏßÄ: 574205320701.dkr.ecr.ap-northeast-2.amazonaws.com/project/flask:karpenter-1.6.0"
            kubectl get pods -n karpenter
            kubectl get crd | grep karpenter
          else
            echo "KARPENTER_READY=false" >> $GITHUB_ENV
            echo "‚ùå Karpenter ÏÑ§Ïπò Ïã§Ìå®"
            echo "Pod ÏÉÅÌÉú:"
            kubectl get pods -n kube-system
            kubectl describe pods -n karpenter | grep -A 10 "Events:" || echo "Ïù¥Î≤§Ìä∏ ÏóÜÏùå"
          fi
          
      - name: Apply Karpenter NodePool configuration
        run: |
            echo "=== NodePool Ï†ÅÏö© ==="
            cat <<EOF | kubectl apply -f -
            apiVersion: karpenter.sh/v1beta1
            kind: NodePool
            metadata:
              name: default
            spec:
              template:
                metadata:
                  labels:
                    karpenter.sh/nodepool: default
                spec:
                  requirements:
                    - key: kubernetes.io/arch
                      operator: In
                      values: ["amd64"]
                    - key: kubernetes.io/os
                      operator: In
                      values: ["linux"]
                    - key: karpenter.sh/capacity-type
                      operator: In
                      values: ["spot", "on-demand"]
                    - key: node.kubernetes.io/instance-type
                      operator: In
                      values: ["t3.medium", "t3.large", "t3.xlarge", "m5.large", "m5.xlarge"]
                  
                  nodeClassRef:
                    apiVersion: karpenter.k8s.aws/v1beta1
                    kind: EC2NodeClass
                    name: default
              
              limits:
                cpu: 1000
                memory: 1000Gi
              
              disruption:
                consolidationPolicy: WhenUnderutilized
                consolidateAfter: 30s
                expireAfter: 30m
            EOF
  
      - name: Apply Karpenter EC2NodeClass configuration
        run: |
            echo "=== EC2NodeClass Ï†ÅÏö© ==="
            cat <<EOF | kubectl apply -f -
            apiVersion: karpenter.k8s.aws/v1beta1
            kind: EC2NodeClass
            metadata:
              name: default
            spec:
              amiFamily: AL2023
              
              # Ïã§Ï†ú ÏÑúÎ∏åÎÑ∑ ID ÏßÅÏ†ë ÏßÄÏ†ï
              subnetSelectorTerms:
                - id: subnet-05debc08a249d8c0d
                - id: subnet-0005ba7fed0b87805
              
              # Ïã§Ï†ú Î≥¥Ïïà Í∑∏Î£π ID ÏßÅÏ†ë ÏßÄÏ†ï
              securityGroupSelectorTerms:
                - id: sg-0f6bacca39650f9e6
              
              role: "KarpenterNodeInstanceProfile"
              
              userData: |
                #!/bin/bash
                /etc/eks/bootstrap.sh $EKS_CLUSTER_NAME
                echo "Node bootstrapped successfully"
              
              blockDeviceMappings:
                - deviceName: /dev/xvda
                  ebs:
                    volumeSize: 20Gi
                    volumeType: gp3
                    encrypted: true
                    deleteOnTermination: true
              
              tags:
                Environment: production
                Team: platform
                Project: flask-app
                karpenter.sh/discovery: $EKS_CLUSTER_NAME
            EOF

      - name: Set secrets for Kubernetes
        run: |
          kubectl delete secret flask-secrets --ignore-not-found
          kubectl create secret generic flask-secrets \
            --from-literal=GEMINI_API_KEY="${{ secrets.GEMINI_API_KEY }}" \
            --from-literal=TAGO_API_KEY="${{ secrets.TAGO_API_KEY }}" \
            --from-literal=API_KEY="${{ secrets.API_KEY }}" \
            --from-literal=FLASK_SECRET_KEY="${{ secrets.FLASK_SECRET_KEY }}" \
            --from-literal=MYSQL_URI="${{ secrets.MYSQL_URI }}" \
            --from-literal=AWS_ACCESS_KEY_ID="${{ secrets.AWS_ACCESS_KEY_ID }}" \
            --from-literal=AWS_SECRET_ACCESS_KEY="${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
            --from-literal=AWS_REGION="${{ secrets.AWS_REGION }}" \
            --from-literal=S3_BUCKET="${{ secrets.S3_BUCKET }}" \
            --from-literal=DYNAMODB_TABLE="${{ secrets.DYNAMODB_TABLE }}"

      - name: Deploy to EKS
        run: |
          kubectl apply -f deployment.yaml
          kubectl apply -f service.yaml

      - name: Wait for deployment to be ready
        run: |
          echo "Waiting for deployment to be ready..."
          kubectl rollout status deployment/flask-app --timeout=300s

      - name: Check Karpenter node provisioning
        run: |
          echo "=== Karpenter Status ==="
          kubectl get nodepool
          kubectl get ec2nodeclass
          kubectl get pods -n karpenter
          echo "=== Node Status ==="
          kubectl get nodes -L karpenter.sh/nodepool
          echo "=== Pending Pods ==="
          kubectl get pods --field-selector=status.phase=Pending

      - name: Wait for LoadBalancer DNS
        id: wait-lb
        run: |
          for i in {1..30}; do
            LB_DNS=$(kubectl get svc flask-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
            if [ -n "$LB_DNS" ]; then
              echo "LB_DNS=$LB_DNS" >> $GITHUB_ENV
              echo "LoadBalancer DNS: $LB_DNS"
              break
            fi
            echo "Waiting for LoadBalancer DNS... ($i/30)"
            sleep 10
          done
          if [ -z "$LB_DNS" ]; then
            echo "LoadBalancer DNS not assigned after 5 minutes!"
            kubectl get svc flask-service -o yaml
            exit 1
          fi

      - name: Health check
        run: |
          echo "Waiting for service to be fully ready..."
          sleep 60
          echo "Running health check on http://$LB_DNS"
          python healthcheck.py http://$LB_DNS

      - name: Show deployment status
        run: |
          echo "=== Deployment Summary ==="
          echo "Application URL: http://$LB_DNS"
          echo "=== Pods Status ==="
          kubectl get pods -l app=flask-app
          echo "=== Service Status ==="
          kubectl get svc flask-service
          echo "=== Deployment Status ==="
          kubectl get deployment flask-app
          echo "=== Karpenter Nodes ==="
          kubectl get nodes -L karpenter.sh/nodepool,node.kubernetes.io/instance-type